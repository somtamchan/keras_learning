{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"cnn_example.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"AfLR4mdyRpL0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":189},"outputId":"2a0daa90-115a-4d96-fa3b-355e99dee62c","executionInfo":{"status":"ok","timestamp":1554638669350,"user_tz":-540,"elapsed":84416,"user":{"displayName":"Yoshimi Sawahata","photoUrl":"","userId":"03373523442075442136"}}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"metadata":{"id":"u4NOlIxvSAkY","colab_type":"code","colab":{}},"cell_type":"code","source":["cp ./gdrive/My\\ Drive/keras-yu/for443.zip ./for443.zip\n","!unzip ./for443.zip"],"execution_count":0,"outputs":[]},{"metadata":{"id":"lXhiliSdShEy","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"eb23ad12-38e4-4610-ff5f-dcc202d341c6","executionInfo":{"status":"ok","timestamp":1554638787933,"user_tz":-540,"elapsed":818,"user":{"displayName":"Yoshimi Sawahata","photoUrl":"","userId":"03373523442075442136"}}},"cell_type":"code","source":["cd for443"],"execution_count":8,"outputs":[{"output_type":"stream","text":["/content/for443\n"],"name":"stdout"}]},{"metadata":{"id":"4zq855dPTHy8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":63},"outputId":"ca3c7d8a-6aed-4af1-aef2-624a66716a6c","executionInfo":{"status":"ok","timestamp":1554638801749,"user_tz":-540,"elapsed":2869,"user":{"displayName":"Yoshimi Sawahata","photoUrl":"","userId":"03373523442075442136"}}},"cell_type":"code","source":["!pwd"],"execution_count":9,"outputs":[{"output_type":"stream","text":["/content/for443\n"],"name":"stdout"}]},{"metadata":{"id":"min4Wh_JRjIl","colab_type":"code","colab":{}},"cell_type":"code","source":["from keras.applications.resnet50 import ResNet50, preprocess_input\n","from keras.preprocessing import image\n","from keras.models import Model\n","from keras.layers import Dense, GlobalAveragePooling2D\n","from keras.layers import Dropout\n","from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping, CSVLogger\n","import time\n","from keras.optimizers import SGD\n","from sklearn import metrics\n","from keras import backend as K\n","\n","import os"],"execution_count":0,"outputs":[]},{"metadata":{"id":"OENuVRg6RjIu","colab_type":"code","colab":{}},"cell_type":"code","source":["TRAIN_DIR = './data/train/'\n","VAL_DIR = './data/validation/'\n","MODEL_OUT_DIR='weights/'\n","LOG_DIR = 'logs/'\n","\n","EPOCHS=100\n","BATCH_SIZE = 16\n","CLASS_NUM = 2\n","THRESHOLD = 0.5"],"execution_count":0,"outputs":[]},{"metadata":{"id":"HI6qhhM2RjI1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":111},"outputId":"b241d52f-d49c-4495-aec6-066cad998076","executionInfo":{"status":"ok","timestamp":1554641006540,"user_tz":-540,"elapsed":18164,"user":{"displayName":"Yoshimi Sawahata","photoUrl":"","userId":"03373523442075442136"}}},"cell_type":"code","source":["base_model = ResNet50(weights='imagenet', include_top=False,)\n","# add a global spatial average pooling layer\n","x = base_model.output\n","x = GlobalAveragePooling2D()(x)\n","# let's add a fully-connected layer\n","x = Dense(1024, activation='relu')(x)\n","#Dropout for Avoid Overfitting\n","x = Dropout(0.5)(x)\n","# and a logistic layer \n","predictions  = Dense(1, activation='sigmoid')(x)\n","# this is the model we will train\n","model = Model(inputs=base_model.input, outputs=predictions)"],"execution_count":26,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/keras_applications/resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n","  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"],"name":"stderr"}]},{"metadata":{"id":"gtaJGngTRjJC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"fcf7344f-b8e1-47e2-e593-2ed3fff3dba7","executionInfo":{"status":"ok","timestamp":1554641154972,"user_tz":-540,"elapsed":778,"user":{"displayName":"Yoshimi Sawahata","photoUrl":"","userId":"03373523442075442136"}}},"cell_type":"code","source":["# prepare data augmentation configuration\n","train_datagen = image.ImageDataGenerator(\n","    preprocessing_function=preprocess_input,\n","    vertical_flip=True,\n","    horizontal_flip=True,)\n","\n","test_datagen = image.ImageDataGenerator(preprocessing_function=preprocess_input,)\n","\n","train_generator = train_datagen.flow_from_directory(\n","        TRAIN_DIR,\n","        target_size=(224, 224),\n","        batch_size=BATCH_SIZE,\n","        class_mode='binary')\n","\n","validation_generator = test_datagen.flow_from_directory(\n","        VAL_DIR,\n","        target_size=(224, 224),\n","        batch_size=BATCH_SIZE,\n","        class_mode='binary')"],"execution_count":30,"outputs":[{"output_type":"stream","text":["Found 100 images belonging to 2 classes.\n","Found 200 images belonging to 2 classes.\n"],"name":"stdout"}]},{"metadata":{"id":"24fq3FNtRjJL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":931},"outputId":"882b2f81-3f07-4110-9697-a8dbe52d8604"},"cell_type":"code","source":["#Set callbacks\n","tb = TensorBoard(log_dir=LOG_DIR)\n","early_stopper = EarlyStopping(patience=5)\n","checkpointer = ModelCheckpoint(\n","    filepath=os.path.join(MODEL_OUT_DIR, 'modelWeights' + '-' + 'epoch:{epoch:03d}-val_acc:{val_acc:.3f}.hdf5'),\n","    verbose=1,\n","    save_best_only=True)\n","csv_logger = CSVLogger(os.path.join(LOG_DIR, 'training_log-' + str(time.time()) + '.log'))\n","\n","#precision\n","def Precision(y_true, y_pred):\n","    true_positives = K.sum(K.cast(K.greater(K.clip(y_true * y_pred, 0, 1), THRESHOLD), 'float32'))\n","    pred_positives = K.sum(K.cast(K.greater(K.clip(y_pred, 0, 1), THRESHOLD), 'float32'))\n","\n","    precision = true_positives / (pred_positives + K.epsilon())\n","    return precision\n","\n","#recall\n","def Recall(y_true, y_pred):\n","    true_positives = K.sum(K.cast(K.greater(K.clip(y_true * y_pred, 0, 1), THRESHOLD), 'float32'))\n","    poss_positives = K.sum(K.cast(K.greater(K.clip(y_true, 0, 1), THRESHOLD), 'float32'))\n","\n","    recall = true_positives / (poss_positives + K.epsilon())\n","    return recall\n","#Model Compiling\n","model.compile(optimizer=SGD(lr=0.0001, momentum=0.9, nesterov=True), loss='binary_crossentropy',metrics=['accuracy',Precision,Recall])\n","\n","# fine-tune the model\n","model.fit_generator(\n","    train_generator,\n","    steps_per_epoch=len(train_generator.filenames) // BATCH_SIZE,\n","    epochs=EPOCHS,\n","    callbacks=[tb, early_stopper, csv_logger, checkpointer],\n","    validation_data=validation_generator,\n","    validation_steps=len(validation_generator.filenames) // BATCH_SIZE)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch 1/100\n","6/6 [==============================] - 177s 29s/step - loss: 0.8928 - acc: 0.3406 - Precision: 0.3337 - Recall: 0.3955 - val_loss: 0.8360 - val_acc: 0.3098 - val_Precision: 0.3400 - val_Recall: 0.4125\n","\n","Epoch 00001: val_loss improved from inf to 0.83600, saving model to weights/modelWeights-epoch:001-val_acc:0.310.hdf5\n","Epoch 2/100\n","6/6 [==============================] - 182s 30s/step - loss: 0.8806 - acc: 0.4688 - Precision: 0.4985 - Recall: 0.5606 - val_loss: 0.8292 - val_acc: 0.3098 - val_Precision: 0.3312 - val_Recall: 0.3569\n","\n","Epoch 00002: val_loss improved from 0.83600 to 0.82924, saving model to weights/modelWeights-epoch:002-val_acc:0.310.hdf5\n","Epoch 3/100\n","6/6 [==============================] - 168s 28s/step - loss: 0.8271 - acc: 0.4894 - Precision: 0.4165 - Recall: 0.4208 - val_loss: 0.7713 - val_acc: 0.4239 - val_Precision: 0.3447 - val_Recall: 0.3813\n","\n","Epoch 00003: val_loss improved from 0.82924 to 0.77130, saving model to weights/modelWeights-epoch:003-val_acc:0.424.hdf5\n","Epoch 4/100\n","6/6 [==============================] - 166s 28s/step - loss: 0.7906 - acc: 0.5425 - Precision: 0.5102 - Recall: 0.4827 - val_loss: 0.7217 - val_acc: 0.5054 - val_Precision: 0.5347 - val_Recall: 0.4307\n","\n","Epoch 00004: val_loss improved from 0.77130 to 0.72166, saving model to weights/modelWeights-epoch:004-val_acc:0.505.hdf5\n","Epoch 5/100\n","6/6 [==============================] - 168s 28s/step - loss: 0.6009 - acc: 0.6224 - Precision: 0.7001 - Recall: 0.4830 - val_loss: 0.7026 - val_acc: 0.5272 - val_Precision: 0.5598 - val_Recall: 0.3450\n","\n","Epoch 00005: val_loss improved from 0.72166 to 0.70255, saving model to weights/modelWeights-epoch:005-val_acc:0.527.hdf5\n","Epoch 6/100\n","4/6 [===================>..........] - ETA: 33s - loss: 0.7705 - acc: 0.5625 - Precision: 0.6420 - Recall: 0.4583 "],"name":"stdout"}]},{"metadata":{"id":"hpAytFUiRjJW","colab_type":"code","colab":{}},"cell_type":"code","source":["model.summary()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2XKo4GRM1Akq","colab_type":"code","colab":{}},"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import cv2\n","from keras import backend as K\n","from keras.preprocessing.image import array_to_img, img_to_array, load_img\n","from keras.models import load_model\n","\n","K.set_learning_phase(1) #set learning phase\n","\n","\n","\n","def Grad_Cam(input_model, x, layer_name):\n","    '''\n","    Args:\n","       input_model: モデルオブジェクト\n","       x: 画像(array)\n","       layer_name: 畳み込み層の名前\n","\n","    Returns:\n","       jetcam: 影響の大きい箇所を色付けした画像(array)\n","\n","    '''\n","\n","    # 前処理\n","    X = np.expand_dims(x, axis=0)\n","\n","    X = X.astype('float32')\n","    preprocessed_input = X\n","\n","\n","    # 予測クラスの算出\n","\n","    predictions = model.predict(preprocessed_input)\n","    class_idx = np.argmax(predictions[0])\n","    class_output = model.output[:, class_idx]\n","\n","\n","    #  勾配を取得\n","\n","    conv_output = model.get_layer(layer_name).output   # layer_nameのレイヤーのアウトプット\n","    grads = K.gradients(class_output, conv_output)[0]  # gradients(loss, variables) で、variablesのlossに関しての勾配を返す\n","    gradient_function = K.function([model.input], [conv_output, grads])  # model.inputを入力すると、conv_outputとgradsを出力する関数\n","\n","    output, grads_val = gradient_function([preprocessed_input])\n","    output, grads_val = output[0], grads_val[0]\n","\n","    # 重みを平均化して、レイヤーのアウトプットに乗じる\n","    weights = np.mean(grads_val, axis=(0, 1))\n","    cam = np.dot(output, weights)\n","\n","\n","    # 画像化してヒートマップにして合成\n","\n","    cam = cv2.resize(cam, (224, 224), cv2.INTER_LINEAR) # 画像サイズは200で処理したので\n","    cam = np.maximum(cam, 0) \n","    cam = cam / cam.max()\n","\n","    jetcam = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)  # モノクロ画像に疑似的に色をつける\n","    jetcam = cv2.cvtColor(jetcam, cv2.COLOR_BGR2RGB)  # 色をRGBに変換\n","    jetcam = (np.float32(jetcam) + x / 2)   # もとの画像に合成\n","\n","    return jetcam"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qLZDjvfA64G3","colab_type":"code","colab":{}},"cell_type":"code","source":["x = img_to_array(load_img('data/validation/cat/cat.1.jpg', target_size=(224,224)))\n","array_to_img(x)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"V6M-OJg067Dv","colab_type":"code","colab":{}},"cell_type":"code","source":["image = Grad_Cam(model, preprocess_input(x), 'activation_196') \n","array_to_img(image)"],"execution_count":0,"outputs":[]}]}